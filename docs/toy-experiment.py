import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import random

# Set random seed for reproducibility
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# --- 幾何與增強函式 (Geometric and Augmentation Functions) ---

def to_polar(x):
    """將笛卡爾座標轉換為極座標 (r, theta)"""
    # x is a torch tensor
    r = torch.sqrt(x[:, 0]**2 + x[:, 1]**2)
    theta = torch.atan2(x[:, 1], x[:, 0])
    return r, theta

def to_cartesian(r, theta):
    """將極座標 (r, theta) 轉換為笛卡爾座標"""
    x = r * torch.cos(theta)
    y = r * torch.sin(theta)
    return torch.stack([x, y], dim=1)

def slerp(p1, p2, t):
    """
    球面線性插值 (Slerp)。在圓流形上，這等價於測地線插值。
    p1, p2: 兩個點的極座標角度 (theta)
    t: 插值係數 (lambda), a tensor of shape [batch_size]
    """
    omega = p2 - p1
    # 處理角度環繞問題
    omega = torch.where(omega > np.pi, omega - 2 * np.pi, omega)
    omega = torch.where(omega < -np.pi, omega + 2 * np.pi, omega)
    return p1 + t * omega

def seki_augmentation(X, y, alpha=1.0):
    """
    SEKI (Symmetry-Equivariant Karcher Interpolation) 的概念實作。
    在圓流形上沿測地線 (弧線) 進行插值。
    X, y are torch tensors.
    """
    # 隨機配對
    indices = torch.randperm(X.size(0))
    X1, y1 = X, y
    X2, y2 = X[indices], y[indices]

    # 生成插值係數 lambda
    lam = torch.tensor(np.random.beta(alpha, alpha, X.size(0)), device=X.device, dtype=X.dtype)

    # 轉換到極座標 (流形上的座標)
    r1, theta1 = to_polar(X1)
    r2, theta2 = to_polar(X2)

    # 在流形上進行插值
    # 1. 半徑 r: 線性插值
    r_new = lam * r1 + (1 - lam) * r2
    # 2. 角度 theta: 球面線性插值 (測地線插值)
    theta_new = slerp(theta1, theta2, lam)

    # 轉換回笛卡爾座標 (環境空間)
    X_new = to_cartesian(r_new, theta_new)
    
    # 標籤插值
    y_new = lam * y1 + (1 - lam) * y2
    
    return X_new, y_new

def mixup_augmentation(X, y, alpha=1.0):
    """傳統 Mixup，在歐式空間中進行線性插值。"""
    lam = np.random.beta(alpha, alpha)
    indices = torch.randperm(X.size(0))
    
    X1, y1 = X, y
    X2, y2 = X[indices], y[indices]
    
    X_new = lam * X1 + (1 - lam) * X2
    y_new = lam * y1 + (1 - lam) * y2
    
    return X_new, y_new

# Define different settings for make_circles
settings = [
    {'n_samples': 400, 'noise': 0.08},  # Original
    {'n_samples': 400, 'noise': 0.16},
    {'n_samples': 400, 'noise': 0.24},
    {'n_samples': 400, 'noise': 0.32},
    {'n_samples': 800, 'noise': 0.08},
    {'n_samples': 800, 'noise': 0.16},
    {'n_samples': 800, 'noise': 0.24},
    {'n_samples': 800, 'noise': 0.32},
    {'n_samples': 1200, 'noise': 0.08},
    {'n_samples': 1200, 'noise': 0.16},
    {'n_samples': 1200, 'noise': 0.24},
    {'n_samples': 1200, 'noise': 0.32},
    {'n_samples': 1600, 'noise': 0.08},
    {'n_samples': 1600, 'noise': 0.16},
    {'n_samples': 1600, 'noise': 0.24},
    {'n_samples': 1600, 'noise': 0.32},
    {'n_samples': 2000, 'noise': 0.08},
    {'n_samples': 2000, 'noise': 0.16},
    {'n_samples': 2000, 'noise': 0.24},
    {'n_samples': 2000, 'noise': 0.32},
]

for setting in settings:
    print(f"\nRunning experiment with n_samples={setting['n_samples']}, noise={setting['noise']}")

    # --- 資料準備 (Data Preparation) ---
    X_np, y_np = make_circles(n_samples=setting['n_samples'], noise=setting['noise'], factor=0.5, random_state=42)
    X_np = StandardScaler().fit_transform(X_np)
    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_np, y_np, test_size=0.5, random_state=42)

    # 轉換為 PyTorch Tensors
    X_train = torch.FloatTensor(X_train_np)
    y_train = torch.FloatTensor(y_train_np)
    X_test = torch.FloatTensor(X_test_np)
    y_test = torch.FloatTensor(y_test_np)

    # --- 視覺化增強效果 (Visualize Augmentation Effects) ---
    n_vis_samples = 50
    X_mixup_vis, _ = mixup_augmentation(X_train[:n_vis_samples], y_train[:n_vis_samples])
    X_seki_vis, _ = seki_augmentation(X_train[:n_vis_samples], y_train[:n_vis_samples])

    plt.style.use('seaborn-v0_8-whitegrid')
    fig, axes = plt.subplots(1, 3, figsize=(21, 6), sharex=True, sharey=True)
    fig.suptitle(f'Comparison of Samples Generated by Different Data Augmentation Methods (n_samples={setting["n_samples"]}, noise={setting["noise"]})', fontsize=16)

    # 原始資料
    axes[0].scatter(X_train_np[:, 0], X_train_np[:, 1], c=y_train_np, cmap=plt.cm.RdBu, edgecolors='k')
    axes[0].set_title('Original Training Data')
    axes[0].set_aspect('equal', 'box')

    # Mixup
    axes[1].scatter(X_train_np[:, 0], X_train_np[:, 1], c=y_train_np, cmap=plt.cm.RdBu, edgecolors='k', alpha=0.3)
    axes[1].scatter(X_mixup_vis.numpy()[:, 0], X_mixup_vis.numpy()[:, 1], c='green', marker='x', s=50, label='Mixup Samples')
    axes[1].set_title('Traditional Mixup (Euclidean Interpolation)')
    axes[1].legend()
    axes[1].set_aspect('equal', 'box')

    # SEKI
    axes[2].scatter(X_train_np[:, 0], X_train_np[:, 1], c=y_train_np, cmap=plt.cm.RdBu, edgecolors='k', alpha=0.3)
    axes[2].scatter(X_seki_vis.numpy()[:, 0], X_seki_vis.numpy()[:, 1], c='purple', marker='x', s=50, label='SEKI Samples')
    axes[2].set_title('SEKI (Geodesic Interpolation on Manifold)')
    axes[2].legend()
    axes[2].set_aspect('equal', 'box')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    # --- 模型訓練與評估 (Model Training and Evaluation) ---

    # 定義 PyTorch 模型
    class MLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = nn.Sequential(
                nn.Linear(2, 32),
                nn.ReLU(),
                nn.Linear(32, 32),
                nn.ReLU(),
                nn.Linear(32, 1),
                nn.Sigmoid()
            )

        def forward(self, x):
            return self.layers(x)

    def train_model(X_train, y_train, augmentation=None, epochs=150, batch_size=32):
        """訓練一個 PyTorch MLP 分類器，可選擇性使用資料增強"""
        model = MLP()
        optimizer = optim.Adam(model.parameters(), lr=0.005)
        criterion = nn.BCELoss() # Binary Cross Entropy, 適合處理軟標籤
        
        train_dataset = TensorDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            model.train()
            for X_batch, y_batch in train_loader:
                y_batch = y_batch.view(-1, 1) # 確保 y 的形狀正確
                
                if augmentation:
                    X_aug, y_aug_soft = augmentation(X_batch, y_batch.squeeze(), alpha=0.4)
                    outputs = model(X_aug)
                    loss = criterion(outputs, y_aug_soft.view(-1, 1))
                else: # ERM
                    outputs = model(X_batch)
                    loss = criterion(outputs, y_batch)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
        return model

    def evaluate_model(model, X_test, y_test):
        model.eval()
        with torch.no_grad():
            outputs = model(X_test)
            predicted = (outputs > 0.5).squeeze().long()
            accuracy = (predicted == y_test.long()).float().mean()
        return accuracy.item()

    print("Starting model training (PyTorch version)...")

    # 1. 基準模型 (ERM)
    model_erm = train_model(X_train, y_train, augmentation=None)
    acc_erm = evaluate_model(model_erm, X_test, y_test)
    print(f"Baseline model (ERM) accuracy: {acc_erm:.4f}")

    # 2. Mixup 模型
    model_mixup = train_model(X_train, y_train, augmentation=mixup_augmentation)
    acc_mixup = evaluate_model(model_mixup, X_test, y_test)
    print(f"Mixup model accuracy: {acc_mixup:.4f}")

    # 3. SEKI 模型
    model_seki = train_model(X_train, y_train, augmentation=seki_augmentation)
    acc_seki = evaluate_model(model_seki, X_test, y_test)
    print(f"SEKI model accuracy: {acc_seki:.4f}")

    # Collect results for summary
    if 'results' not in globals():
        results = []
    results.append({
        'n_samples': setting['n_samples'],
        'noise': setting['noise'],
        'acc_erm': acc_erm,
        'acc_mixup': acc_mixup,
        'acc_seki': acc_seki
    })

print("\nAll experiments completed. Summary of results:")
for res in results:
    # Colorful summary per experiment (ANSI escape codes)
    RESET = "\033[0m"
    BOLD = "\033[1m"
    BLUE = "\033[34m"
    YELLOW = "\033[33m"
    MAGENTA = "\033[35m"
    GREEN = "\033[32m"

    acc_erm = res['acc_erm']
    acc_mixup = res['acc_mixup']
    acc_seki = res['acc_seki']
    max_acc = max(acc_erm, acc_mixup, acc_seki)
    eps = 1e-8

    def fmt(color, val):
        s = f"{val:.4f}"
        if abs(val - max_acc) < eps:
            return f"{BOLD}{GREEN}{s}{RESET}"
        return f"{color}{s}{RESET}"

    print(
        f"n_samples={res['n_samples']}, noise={res['noise']} => "
        f"ERM: {fmt(BLUE, acc_erm)}  "
        f"Mixup: {fmt(YELLOW, acc_mixup)}  "
        f"SEKI: {fmt(MAGENTA, acc_seki)}"
    )

print(f"ERM wins: {sum(1 for r in results if r['acc_erm'] >= r['acc_mixup'] and r['acc_erm'] >= r['acc_seki'])} times")
print(f"Mixup wins: {sum(1 for r in results if r['acc_mixup'] >= r['acc_erm'] and r['acc_mixup'] >= r['acc_seki'])} times")
print(f"SEKI wins: {sum(1 for r in results if r['acc_seki'] >= r['acc_erm'] and r['acc_seki'] >= r['acc_mixup'])} times")

print("\nExperiment Summary:")
print("In the PyTorch implementation, models can be trained with soft labels, and data augmentation is dynamic.")
print("This usually makes the effects of SEKI and Mixup more pronounced, especially in generalization and model robustness.")
print("SEKI is expected to perform better or more stably than Mixup due to its augmentation method being closer to the intrinsic geometry of the data.")